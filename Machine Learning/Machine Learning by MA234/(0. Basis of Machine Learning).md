## Machine Learning

### 1.1 Categories of learning tasks

- Supervised.
  Given the information of data and labels, training for predict.

  - Classification
    Predict the label.

  - Regression.
    Predict the value.



- Unsupervised.
  Learning with no labels.

  Density estimatioin, Clustering, Dimension reduction.
  
- Semi-supvised.
  The methods learning with missing data, hidden parameters.

  e.g., EM, self-supervised learning, Inpainting.

- Reinforcement learning
  Play games, auto-steering, robotics.

### 1.2 Notations

- Input, Output Space
  Using $\mathcal{X, Y}$ represente.


### 1.3 Learning model in mathematics

#### What's learning
The task of learning is a search task in decision function space, using the loss function as the metric, tending to minimize the expectation of average risk in the whole space.

$$\mathcal{F} = \{f_\theta\}\quad or \quad \mathcal{F} = \{P_\theta\}$$

#### Loss function

And we use the loss function to check the distance.
- 0-1 loss
  For a classifier problem, if the predict is different with real, get number 1 as loss.
  $$L(y_i, f(x_i)) = 1 - I_{y_i}[f(x_i)] = I_{y_i \ne f(x_i)} $$
- Square loss
  
- Absolute loss
  
- Cross-entroy
  $$L(y, f(x)) = -y\ln f - (1 - y)\ln f$$
  Note that, if $y$ has different label with $f$, the loss will tends to infinity.

- Decision function 

Note that minimize the expectation in hypothesis space is impossible, thus we introduce the following concepts.

#### Risk Minimization

- [Experience]().

  The expectational risk $R_{exp}$ is defined as the expectation of error in data space.

  > It's clearly that the $R_{exp}$ is impossible to calculate without of all the real world data.
  >
- [Empirical]().

  Considering the empirical risk, which means the average error in training set.

  > According to large number theory,
  >
  > $$
  > R_{emp} \to R_{exp}
  > $$
  >
- [Structural Risk](Struct).

  Considering the risk defined below can be decreased by increasing the complexity model.
  
  Thus we can't just add too many parameters to our model. We need also to consider the complexity of model.

  Thus, define [Structural risk minimization]() by

  $$
  {SRM}(f) = R_{emp} + \lambda J(f)
  $$

  Here, we add a penalty $J(f)$, which is a functional of $f$, which can represents the complexity of $f$.


### 1.4 Single Model Assement

In this section, note our model generated by learning as $\hat f$.

#### Error in data

- Training error.

  Training error is defined as the empirical risk of model in training set.


- Test error.

  Test error is defined as the $R_{emp}$ in test set.

  > In particular, for the 0-1 classification problem, we can define,
  >
  > - Error := The probability of corect classification.
  > - Accuracy := The probability of incorrect classification.
  > - Error + Accuracy = 1
  >



#### Generalization error (Error in hypothesis space)

Define

$$R_{exp}(\hat f) = E_P[L(y, \hat f)]$$

which assess the ability of predicting unknow input.

### 1.5 Model Selection

#### Selection by minimizing Regularization

$$\min_{f \in F} SRM(f) = \min_{f \in \mathcal F} \frac 1 n \sum_{i = 1}^n L(y_i, f(x_i)) + \lambda J(f)$$

#### Cross-validation

- Simple CV
  Randomly split the data into two subsets.

- K fold CV
  Randomly split the data uniformly into k subsets and leave a subset as the validation set.

- Leave-one-out CV
  K = n in the previous case. Which means we just use every point as a single dataset, and leave only one point for validation.


